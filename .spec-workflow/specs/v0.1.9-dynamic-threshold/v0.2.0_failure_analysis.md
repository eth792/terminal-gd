# v0.2.0 Failure Analysis - Math.min() Normalization Fix

**日期**: 2025-11-19
**测试运行**: `run_20251119_22_07`
**实验状态**: ❌ **失败 - 未达回滚阈值**

---

## 实验目标

修复 v0.2.0 首次实施的 Math.max() 归一化错误，改用 Math.min() 以奖励完整子串匹配。

**假设**: Math.max() 过度惩罚长度差异，导致 -31% 回归。改用 Math.min() 后，Exact 应恢复至 baseline (71) 或更高。

---

## 测试结果对比

| 指标 | Baseline (v0.1.7b) | Math.max (首次) | Math.min (修复) | 目标 | 状态 |
|------|-------------------|----------------|----------------|------|------|
| **Exact** | **71** | 49 (-31%) | **56 (-21%)** | ≥99 | ❌ **失败** |
| Review | 38 | 59 | 40 (+5%) | <50 | ⚠️ 边缘 |
| Fail | 113 | 114 | 126 (+11%) | - | ❌ 恶化 |
| Auto-pass rate | 32.0% | 22.1% | 25.2% | 44.6% | ❌ |

### 失败原因分布（Top 3）

| 原因 | Math.max | Math.min | 变化 |
|------|----------|----------|------|
| FIELD_SIM_LOW_PROJECT | 69 | **57** | -12 ✅ |
| DELTA_TOO_SMALL | 21 | 40 | +19 ❌ |
| SUPPLIER_HARD_REJECT | 18 | 31 | +13 ❌ |

### 关键发现

1. **Math.min() 部分生效**：
   - Exact 从 49 → 56 (+7)，说明归一化方向正确
   - FIELD_SIM_LOW_PROJECT 下降 -12，F2 改善明显

2. **但引入新问题**：
   - DELTA_TOO_SMALL 激增 +19（21 → 40）
   - SUPPLIER_HARD_REJECT 增加 +13（18 → 31）
   - 说明算法不稳定，过度提升 F2 反而降低了判别能力

3. **Fail 总数恶化**：113 → 126 (+13)，整体质量下降

---

## Linus Five-Layer 根因分析

### 层 1: 数据结构问题 ❌

**问题**: LCS + Jaccard + Levenshtein 三算法相互干扰

```typescript
// 当前权重配置
return 0.2 * lev + 0.4 * jac + 0.4 * lcs;
```

**根因**:
- Jaccard 基于 bigram token overlap（容忍顺序）
- LCS 基于连续子串匹配（容忍长度差异）
- **两者都倾向于给长度不匹配的字符串高分**，导致 false positive

**案例**（推测）:
```
OCR:  "武汉市花山185地块项目" (12 chars)
DB1:  "武汉市花山185地块项目新建住宅工程" (19 chars) [正确答案]
DB2:  "武汉市花山生态新城建设项目" (14 chars) [错误答案]

Baseline (50% Lev + 50% Jac):
- DB1: score = 0.75 (高 Lev，高 Jac) ✅ 正确排序
- DB2: score = 0.60 (低 Lev，中等 Jac)

v0.2.0 (20% Lev + 40% Jac + 40% LCS):
- DB1: score = 0.80 (LCS = 1.0) ✅ 仍然最高
- DB2: score = 0.75 (LCS = 0.85, Jac = 0.65) ⚠️ 分数接近
- Delta = 0.05 < 0.03 threshold → DELTA_TOO_SMALL ❌
```

**结论**: LCS 的"容忍子串匹配"特性，反而降低了 Top-1 和 Top-2 的区分度，导致 DELTA_TOO_SMALL 激增。

---

### 层 2: 特殊情况识别 ⚠️

**已知问题**（未修复）:
1. 标点符号污染（单引号 `''`、括号 `()`）
2. DB 额外信息（如 "（光谷P（2023）028地块）"）
3. OCR 截断（提取不完整）

**为什么修复 Math.min() 无法解决这些问题？**

因为标点符号和额外信息会同时污染 Lev、Jac、LCS 三个算法：
- Lev: 插入/删除成本增加
- Jac: token 集合被稀释
- LCS: 连续匹配被打断

**即使 LCS 容忍长度差异，标点符号仍会破坏连续子串匹配。**

---

### 层 3: 复杂度审查 ❌

**引入的复杂性**:
- LCS 算法: O(N*M) 动态规划
- 三算法混合: 需要调优 3 个权重参数

**实际收益**:
- Exact +7（56 vs 49）
- 但代价是 DELTA_TOO_SMALL +19，净收益可能为负

**Linus 判断**:
> "This is over-engineering. We added O(N*M) complexity for +7 Exact, but lost -19 due to delta issues. The juice is not worth the squeeze."

---

### 层 4: 破坏性分析 ❌

**向后兼容性**: ✅ API 未破坏
**性能回退**: ❌ Exact 仍下降 21%（56 vs 71 baseline）

**违反 "Never break userspace" 原则**:
虽然是内部算法变更，但性能回退等同于功能破坏。

---

### 层 5: 实用性验证 ❌

**这是真问题还是臆想？**

**真问题**: F2 比 F1 低 0.142（数据证实）
**但解决方案错误**: LCS 算法不是银弹

**为什么 LCS 不是解决方案？**

1. **F2 低的真正原因**（v0.2.0_ultrathink.md 假设 2）:
   - OCR 提取不完整（根本问题）
   - DB 数据格式不统一（根本问题）
   - 标点符号污染（配置问题）

2. **LCS 只能缓解症状，无法治本**:
   - 即使 LCS 容忍长度差异，OCR 提取错误仍会导致匹配失败
   - 即使 LCS 归一化正确，标点符号仍会打断连续匹配

**Linus 判断**:
> "We're using a tank to kill a fly. The real problem is normalization (punctuation) and data quality (DB inconsistency), not the similarity algorithm."

---

## 技术洞察

### 为什么 Math.min() 比 Math.max() 好，但仍然失败？

**Math.max() 的问题**:
```typescript
lcsRatio("ABC", "ABCDEF")
  = LCS(3) / max(3, 6)
  = 3/6 = 0.5 ❌ 过度惩罚
```

**Math.min() 的改进**:
```typescript
lcsRatio("ABC", "ABCDEF")
  = LCS(3) / min(3, 6)
  = 3/3 = 1.0 ✅ 奖励完整匹配
```

**但为什么还是失败？**

因为 **LCS + Jaccard 的组合权重过高** (0.4 + 0.4 = 0.8)，导致：
- **正确答案 (DB1)** 的 LCS=1.0 和 Jac=0.9 → score=0.85
- **错误答案 (DB2)** 的 LCS=0.85 和 Jac=0.75 → score=0.78
- **Delta = 0.07**，虽然 > 0.03，但如果 Lev 差异大，可能被 Lev 拉低

**关键矛盾**:
- 降低 Lev 权重 (0.2) 是为了减少长度差异惩罚
- 但这同时降低了 Lev 的判别能力（Lev 对字符级差异最敏感）
- 结果：F2 改善了（FIELD_SIM_LOW_PROJECT -12），但判别能力下降（DELTA_TOO_SMALL +19）

---

## 实验结论

### ❌ Math.min() 修复失败的原因

1. **算法设计缺陷**: LCS 的"容忍子串匹配"特性降低了判别能力
2. **权重配置不当**: 过度依赖 Jac + LCS，忽视了 Lev 的价值
3. **未解决根本问题**: 标点符号、DB 数据质量仍未处理

### ✅ 得到的教训（Linus 式）

1. **"Good taste" 原则**:
   - 好算法应该消除特殊情况，而不是用复杂逻辑去处理特殊情况
   - LCS 是为了"容忍长度差异"，但这本身就是特殊情况的补丁

2. **"Practical, not theoretical" 原则**:
   - 理论上 LCS 容忍子串匹配，但实际数据中标点符号会打断匹配
   - 应该先修复标点符号（配置问题），而不是引入复杂算法

3. **"Never break userspace" 原则**:
   - 即使是内部算法优化，也不能导致性能回退
   - Exact 下降 21% 等同于破坏用户体验

---

## 下一步建议

### 推荐方案: 回滚 + 数据驱动重新设计

**Phase 1: 立即回滚**
- 恢复 baseline 算法（仅 Lev + Jac，50%/50%）
- 移除 LCS 相关代码

**Phase 2: 标点符号修复（单独实验）**
1. 更新 `normalize.user.json`:
   - 添加单引号 `[''']` 移除规则
   - 添加括号 `[()（）【】]` 移除规则（可选）
2. 运行测试，评估收益
3. 如果 Exact +5-10，提交；否则回滚

**Phase 3: 案例研究（必做）**
1. 读取 Top 10 失败案例的实际 OCR/DB 文本
2. 手工分析为什么匹配失败
3. 基于真实案例设计算法改进
4. 严格测试每个变更（一次只改一处）

**Phase 4: 重新评估 +28 Exact 目标**
- 如果标点符号修复后仍无法达到 71 → 99
- 考虑降低目标，或转攻其他优化方向（F1, Rule 7）

---

## 代码变更记录

**文件**: `packages/ocr-match-core/src/match/similarity.ts`

**变更**:
```diff
// Line 97-99
- const maxPossible = Math.max(len1, len2);
- return maxPossible === 0 ? 1.0 : maxLen / maxPossible;
+ const minPossible = Math.min(len1, len2);
+ return minPossible === 0 ? 1.0 : maxLen / minPossible;
```

**变更理由**: 修复 Math.max() 归一化错误，奖励完整子串匹配

**测试结果**: Exact 56 (-21% vs baseline 71)，未达标，需回滚

---

**作者**: Claude (Linus Torvalds 视角)
**方法论**: Ultrathink Five-Layer Analysis
**结论**: ❌ 过度设计，未解决根本问题，建议回滚
